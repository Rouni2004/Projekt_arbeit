{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Der alte Code",
   "id": "1da610f33840a2ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# === XML-Dateien einlesen ===\n",
    "xml_files = [\"moduldb-pi2.xml\", \"Modulbook_pi2_de.xml\"]\n",
    "docs = []\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    with open(xml_file, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        xml_content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(xml_content, \"xml\")\n",
    "    modules = soup.find_all(\"moduleheader\")\n",
    "\n",
    "    for module in modules:\n",
    "        title = module.find(\"title\").text.strip() if module.find(\"title\") else \"Kein Titel\"\n",
    "        cid = module.find(\"cid\").text.strip() if module.find(\"cid\") else \"Kein K√ºrzel\"\n",
    "        cp = module.find(\"cp\").text.strip() if module.find(\"cp\") else \"?\"\n",
    "        convenor = module.find(\"convenor\").text.strip() if module.find(\"convenor\") else \"?\"\n",
    "\n",
    "        types = module.find_all(\"type\")\n",
    "        ctypes = \", \".join(t.text for t in types) if types else \"Unbekannt\"\n",
    "\n",
    "        content = f\"\"\"\n",
    "Modul: {title}\n",
    "K√ºrzel: {cid}\n",
    "Leistungspunkte (ECTS): {cp}\n",
    "Verantwortlich: {convenor}\n",
    "Veranstaltungstyp(en): {ctypes}\n",
    "\"\"\"\n",
    "        docs.append(Document(page_content=content.strip()))\n",
    "# Debug: Beispielinhalt zeigen\n",
    "if docs:\n",
    "    print(\"üìÑ Beispiel-Inhalt:\\n\", docs[0].page_content[:500])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Keine Module gefunden.\")\n",
    "# === Dokumente splitten ===\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "# === Embeddings & Chroma ===\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = Chroma.from_documents(chunks, embeddings)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "# === LLM √ºber Ollama (Server) ===\n",
    "llm = OllamaLLM(\n",
    "    base_url=\"http://134.96.217.20:53100\",\n",
    "    model=\"llama3-70b\",\n",
    "    temperature=0.5,\n",
    "    top_p=0.8,\n",
    "    top_k=10,\n",
    "    repeat_penalty=1.1,\n",
    "    presence_penalty=1.2\n",
    ")\n",
    "# === Prompt ===\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"Hier ist ein Auszug aus Moduldaten:\\n\"\n",
    "        \"{context}\\n\\n\"\n",
    "        \"Beantworte bitte diese Frage auf einfache Weise:\\n\"\n",
    "        \"{question}\\n\\n\"\n",
    "        \"Antwort:\"\n",
    "    )\n",
    ")\n",
    "# === QA-Kette ===\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "# === Chat starten ===\n",
    "print(\"ü§ñ Chatbot gestartet! Tipp 'exit' oder 'quit' zum Beenden.\\n\")\n",
    "while True:\n",
    "    user_input = input(\"Du: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"ü§ñ Bot: Auf Wiedersehen!\")\n",
    "        break\n",
    "    try:\n",
    "        response = qa_chain.run(user_input)\n",
    "        print(\"ü§ñ Bot:\", response)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fehler: {e}\")"
   ],
   "id": "e78d8f5964975170"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Der neu Code",
   "id": "90d65979316ac688"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Import von LangChain-Komponenten und externen Tools\n",
    "\n",
    "Zu Beginn des Skripts werden alle notwendigen Bibliotheken und Module importiert, die f√ºr den Aufbau des semantischen Frage-Antwort-Systems ben√∂tigt werden.\n"
   ],
   "id": "100d17f45ce6a860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import von langchain-Komponenten und externen Tools\n",
    "from langchain.docstore.document import Document  # Datenstruktur f√ºr Dokumente\n",
    "from langchain.prompts import PromptTemplate  # F√ºr benutzerdefinierte Prompts\n",
    "from langchain.chains import RetrievalQA  # Frage-Antwort-Kette auf Basis von Retrieval\n",
    "from langchain_community.vectorstores import Chroma  # Vektor-Datenbank (Chroma)\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Text-Splitter f√ºr Chunking\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Embedding-Modell von HuggingFace\n",
    "from langchain_ollama import OllamaLLM  # Lokales Ollama-LLM\n",
    "from bs4 import BeautifulSoup  # F√ºr XML-Parsing"
   ],
   "id": "e203573daebe4458"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "  Inhalte aus Textdatei laden und vorbereiten\n",
    "\n",
    "In diesem Schritt wird eine lokal gespeicherte Textdatei (`Inhalte.txt`) eingelesen, die formatierte Modulinhalte enth√§lt.\n",
    "Die Datei wird blockweise aufgeteilt, wobei jeder Block einem Modul entspricht. Diese Bl√∂cke werden anschlie√üend als `Document`-Objekte gespeichert.\n"
   ],
   "id": "c190f4125e501163"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Inhalte.txt laden ===\n",
    "docs = []  # Liste f√ºr Hauptdokumente\n",
    "chunks = []  # Liste f√ºr vorbereitete Text-Chunks\n",
    "\n",
    "# Inhalte.txt vollst√§ndig einlesen\n",
    "with open(\"Inhalte.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Aufteilen in einzelne Modulbl√∂cke\n",
    "blocks = content.split(\"============================================================\")\n",
    "for block in blocks:\n",
    "    block = block.strip()\n",
    "    if len(block) > 100:  # Filter f√ºr leere oder zu kleine Bl√∂cke\n",
    "        docs.append(Document(page_content=block))\n",
    "\n",
    "print(f\"üìÑ Datei 'Inhalte.txt' geladen. Module erkannt: {len(docs)}\")"
   ],
   "id": "467ca99b9628827d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##  Chatbot f√ºr Moduldaten (Text + XML) mit Ollama und LangChain\n",
    "\n",
    "Dieses Skript kombiniert Inhalte aus einer Textdatei (`Inhalte.txt`) und mehreren XML-Dateien (`moduldb-pi2.xml`, `modulbook_pi2_de.xml`), verarbeitet sie in Chunks, speichert sie in einer semantischen Vektordatenbank und nutzt ein lokales LLM (z.‚ÄØB. LLaMA 3 √ºber Ollama), um interaktive Antworten auf Benutzerfragen zu geben.\n",
    "\n",
    "---\n",
    "\n",
    "###  Inhalte.txt laden\n",
    "\n",
    "Die Datei `Inhalte.txt` wird eingelesen und anhand einer Trennlinie (`===...`) in Bl√∂cke unterteilt. Jeder Block stellt ein Modul dar."
   ],
   "id": "1e4deb6a20244eb0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Chunking vorbereiten ===\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=60)\n",
    "\n",
    "# Manual chunking mit Modulnamen zur Orientierung\n",
    "for doc in docs:\n",
    "    first_line = doc.page_content.split(\"\\n\")[0]  # z.B. \"Modul: Betriebssystemeinf√ºhrung\"\n",
    "    vorlesung = first_line.split(\":\", 1)[1].strip().upper()\n",
    "    for chunk_text in splitter.split_text(doc.page_content):\n",
    "        full_chunk = f\"{vorlesung}\\n{chunk_text}\\n{vorlesung}\"  # Modulname oben & unten f√ºr Kontext\n",
    "        chunks.append(Document(\n",
    "            page_content=full_chunk,\n",
    "            metadata={\"modul\": vorlesung.lower()}  # kann sp√§ter f√ºr gezielte Filterung verwendet werden\n",
    "        ))"
   ],
   "id": "f0d3c1772b8655ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Kurz√ºbersicht: Modul-Chatbot mit XML + Ollama\n",
    "\n",
    "- **XML-Dateien laden** (`moduldb-pi2.xml`, `modulbook_pi2_de.xml`) und per `BeautifulSoup` relevante Felder aus `<moduleheader>`-Bl√∂cken extrahieren.\n",
    "- Ergebnisse als `Document`-Objekte speichern.\n"
   ],
   "id": "c630efc8b44e73ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === XML-Dateien einlesen ===\n",
    "xml_files = [\"moduldb-pi2.xml\", \"modulbook_pi2_de.xml\"]\n",
    "docs = []  # XML-Daten √ºberschreiben hier absichtlich vorherige docs-Liste\n",
    "\n",
    "# XML parsen mit BeautifulSoup\n",
    "for xml_file in xml_files:\n",
    "    with open(xml_file, \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        xml_content = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(xml_content, \"xml\")\n",
    "    modules = soup.find_all(\"moduleheader\")  # Module sind unter <moduleheader> gelistet\n",
    "\n",
    "    for module in modules:\n",
    "        # Extraktion typischer Felder aus dem Modul\n",
    "        title = module.find(\"title\").text.strip() if module.find(\"title\") else \"Kein Titel\"\n",
    "        cid = module.find(\"cid\").text.strip() if module.find(\"cid\") else \"Kein K√ºrzel\"\n",
    "        cp = module.find(\"cp\").text.strip() if module.find(\"cp\") else \"?\"\n",
    "        convenor = module.find(\"convenor\").text.strip() if module.find(\"convenor\") else \"?\"\n",
    "\n",
    "        types = module.find_all(\"type\")\n",
    "        ctypes = \", \".join(t.text for t in types) if types else \"Unbekannt\"\n",
    "\n",
    "        content = f\"\"\"\n",
    "Modul: {title}\n",
    "K√ºrzel: {cid}\n",
    "Leistungspunkte (ECTS): {cp}\n",
    "Verantwortlich: {convenor}\n",
    "Veranstaltungstyp(en): {ctypes}\n",
    "\"\"\"\n",
    "        docs.append(Document(page_content=content.strip()))  # Wieder als LangChain-Dokument speichern\n",
    "\n",
    "print(f\"{len(docs)} Dokumente aus xml Dateien extrahiert.\")\n",
    "\n",
    "# XML-Daten auch als Chunks splitten und hinzuf√ºgen\n",
    "chunks.extend(splitter.split_documents(docs))"
   ],
   "id": "257b0e5e47ee0f72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Vektordatenbank, LLM und QA-Chat starten\n",
    "\n",
    "Dieser Abschnitt bildet das Herzst√ºck des Frage-Antwort-Systems.\n",
    "Er verbindet die vorbereiteten Text-Chunks mit einem Vektor-Index und einem lokal laufenden LLM-Modell √ºber Ollama.\n",
    "\n",
    "---\n",
    "\n",
    "####  Embeddings & Vektordatenbank\n",
    "\n",
    "Die vorbereiteten Textabschnitte (`chunks`) werden mit dem hochwertigen Modell `multi-qa-mpnet-base-dot-v1` aus der `sentence-transformers`-Reihe in Vektoren umgewandelt.\n",
    "Diese Vektoren werden in einer Chroma-Datenbank gespeichert und stehen f√ºr semantische Suche zur Verf√ºgung.\n"
   ],
   "id": "7e16504d398ecded"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Vektordatenbank aufbauen ===\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\")  # Hochwertiges QA-Embedding-Modell\n",
    "db = Chroma.from_documents(chunks, embeddings)  # Chroma-Vektordatenbank aufbauen\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 7})  # Suche nach den 7 √§hnlichsten Chunks"
   ],
   "id": "d26590d1a8bf096b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  LLM starten (Ollama)\n",
    "\n",
    "Anbindung eines lokalen Large Language Models √ºber den **Ollama-Server**.\n",
    "Hier wird das Modell `llama3-70b` verwendet, das leistungsf√§hig genug ist, um komplexe Fragen auf Basis von Moduldaten zu beantworten.\n",
    "\n"
   ],
   "id": "f8bac7197ca62b22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === LLM starten ===\n",
    "llm = OllamaLLM(\n",
    "    base_url=\"http://134.96.217.20:53100\",  # Ollama API-Adresse\n",
    "    model=\"llama3-70b\",  # Verwendetes Modell\n",
    "    temperature=0.3  # Kreativit√§t / Zuf√§lligkeit der Antwort\n",
    ")"
   ],
   "id": "fad2dcd20a36cda2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ô∏è Prompt-Definition & Start der QA-Kette\n",
    "\n",
    "####  PromptTemplate\n",
    "Ein benutzerdefinierter Prompt wird erstellt, der das LLM anweist, Fragen **ausschlie√ülich auf Basis der gelieferten Modultexte** zu beantworten.\n",
    "Falls keine Antwort im Text enthalten ist, soll das Modell dies auch so zur√ºckmelden.\n"
   ],
   "id": "75f23055fe5e9152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# === Prompt f√ºr Frage-Antwort-System ===\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=(\n",
    "        \"Hier ist ein Auszug aus Modulbeschreibungen:\\n\"\n",
    "        \"{context}\\n\\n\"\n",
    "        \"Beantworte die folgende Frage **nur auf Basis dieser Informationen**. \"\n",
    "        \"Wenn die Antwort nicht im Text steht, sage: 'Nicht enthalten'.\\n\\n\"\n",
    "        \"Frage: {question}\\n\\n\"\n",
    "        \"Antwort:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# === QA-Kette starten ===\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",  # alle Chunks \"stuffed\" in ein Prompt\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")"
   ],
   "id": "99dbdb25574f092"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interaktive Chat-Schleife & Testabfrage\n",
    "\n",
    "#### üßë‚Äçüíª Chat mit dem LLM\n",
    "Der Benutzer interagiert direkt √ºber die Konsole mit dem Frage-Antwort-System.\n",
    "Die Eingaben werden an die `RetrievalQA`-Kette √ºbergeben, die relevante Textabschnitte sucht und das LLM zur Antwort nutzt.\n"
   ],
   "id": "cf08a87444db0197"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Run Chatbot\n",
    "# === Interaktive Schleife ===\n",
    "print(\"\\nü§ñ Chatbot gestartet! Tipp 'exit' oder 'quit' zum Beenden.\\n\")\n",
    "while True:\n",
    "    user_input = input(\"Du: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"ü§ñ Bot: Auf Wiedersehen!\")\n",
    "        break\n",
    "    try:\n",
    "        response = qa_chain.invoke({\"query\": user_input})  # Achtung: invoke braucht {\"query\": ...}\n",
    "        print(\"ü§ñ Bot:\", response[\"result\"])  # Zeigt nur den Antworttext\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Fehler:\", e)\n",
    "\n",
    "## Test\n",
    "# Einzelne Testabfrage (nicht interaktiv)\n",
    "retriever.invoke(\"Literatur Betriebssystemeinf√ºhrung?\")  # Ruft relevante Chunks ab (kein LLM)\n"
   ],
   "id": "1d46168d188dd18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
